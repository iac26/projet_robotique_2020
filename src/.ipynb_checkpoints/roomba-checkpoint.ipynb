{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:40px; font-weight:900;\"> TITRE </div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Vision</a></span><ul class=\"toc-item\"><li><span><a href=\"#External-libraries\" data-toc-modified-id=\"External-libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>External libraries</a></span></li><li><span><a href=\"#Color-detection\" data-toc-modified-id=\"Color-detection-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Color detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-HSV-color-space\" data-toc-modified-id=\"The-HSV-color-space-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>The HSV color space</a></span></li><li><span><a href=\"#Range-finding-script\" data-toc-modified-id=\"Range-finding-script-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Range finding script</a></span></li></ul></li><li><span><a href=\"#Polygon-extraction\" data-toc-modified-id=\"Polygon-extraction-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Polygon extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Working-principle\" data-toc-modified-id=\"Working-principle-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Working principle</a></span></li><li><span><a href=\"#Test-script\" data-toc-modified-id=\"Test-script-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Test script</a></span></li></ul></li><li><span><a href=\"#Robot-detection\" data-toc-modified-id=\"Robot-detection-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Robot detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-the-robot\" data-toc-modified-id=\"Find-the-robot-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Find the robot</a></span></li><li><span><a href=\"#Position,-orientation-and-scale\" data-toc-modified-id=\"Position,-orientation-and-scale-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Position, orientation and scale</a></span></li><li><span><a href=\"#The-code\" data-toc-modified-id=\"The-code-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>The code</a></span></li></ul></li><li><span><a href=\"#Obstacles-detection\" data-toc-modified-id=\"Obstacles-detection-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Obstacles detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dilatation\" data-toc-modified-id=\"Dilatation-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Dilatation</a></span></li><li><span><a href=\"#Overlapping-obstacles-merge\" data-toc-modified-id=\"Overlapping-obstacles-merge-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Overlapping obstacles merge</a></span></li></ul></li><li><span><a href=\"#Code-encapsulation\" data-toc-modified-id=\"Code-encapsulation-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Code encapsulation</a></span></li></ul></li><li><span><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Global navigation</a></span></li><li><span><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Filtering</a></span></li><li><span><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Local navigation</a></span></li><li><span><a href=\"#Main-code\" data-toc-modified-id=\"Main-code-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Main code</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External libraries\n",
    "The purpose of this computer vision module is to provide the robot with information about the world globally surrounding it.\n",
    "\n",
    "The computer vision module must be able to detect:\n",
    "* The robot, it's position and orientation\n",
    "* The obstacles\n",
    "* The objectives\n",
    "* The world scale\n",
    "\n",
    "In order to detect these different objects, we decided to use different colors. And detect color ranges in the image. These colors will then be transformed to shapes. That allows us to extract the contours of obstacles or then centroids of targets. it will also allow us to find the orientation of the robot.\n",
    "\n",
    "We use the OpenCV library. This library requires numpy for it's data types and we also added the matplotlib library to display examples and debug information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#COLORS \n",
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (255, 0, 0)\n",
    "LIGHTBLUE = (255, 127, 0)\n",
    "TURQUOISE = (255, 255, 0)\n",
    "PINK = (255, 0, 255)\n",
    "ORANGE = (0, 127, 255)\n",
    "YELLOW = (0, 255, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color detection\n",
    "\n",
    "The idea behind our color detection is to define each of our interesting colors as a range of values. When using opneCV's default BGR color space, our colors would need to be defined as a volume in the color space. This is necessary because there are always variations in the color seen by the camera. Find and tunes these volumes in the BGR color space is difficult because BGR is not an intuitive way of describing colors. \n",
    "We decided to use the HSV color space because it is a more \"human intuitive\" representation of the colors.\n",
    "\n",
    "\n",
    "### The HSV color space\n",
    "The HSV color space represents the colors in three dimensions like BGR. But it has a more human intuitive approach as the question of \"which color it is?\" (Hue) can be answered on one dimention. The two others are \"the ammount of coloration\" (Saturation and the \"darkness\" (Value). The HSV color space is a cylindrical coordinates representation of the colors as it is illustrated here:\n",
    "\n",
    "<img src=\"images/hsv_cyl.png\" alt=\"The HSV color space\" width=\"600\"/>\n",
    "\n",
    "In OpenCV, we can easily convert an image to HSV using the color conversion function ```frame = cv2.cvtColor(imag, cv2.COLOR_BGR2HSV)```. The result will be an image with pixels encoded with three values: Hue (between 0 and 179), Saturation and Value (both between 0 and 255).\n",
    "Thanks to this representation we can easily select colors by taking just a \"slice of cylinder\" defined by ```color_low = np.array([h_low, s_low, v_low], dtype=np.uint8)``` and ```color_high = np.array([h_high, s_high, v_high], dtype=np.uint8)```. Such a simple selection volume allows us to use the color selection function ```hsv = cv2.inRange(frame,  color_low, color_high)```. This function will return a mask that is white whenever a pixel is in the range of colors.\n",
    "\n",
    "One of the problems with the HSV color range detection method is that the color is at the 179 -> 0 boundary. so wee need to choose between the yellowish red which is on the 0 side or the pinkish red which is on the 179 part. We decided not to handle the detection for all hues of red as it would add complexity and we can simply use another color :)\n",
    "\n",
    "\n",
    "### Range finding script\n",
    "We wrote a simple range finding script to help us find the color boundaries for different interesting colors.\n",
    "The script simply displays the webcam image with the inRange color mask as overlay. The range can be tuned in real time using sliders. This script allows us to tune the color detection for optimal performance in different ligntning environement and for different colors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "     \n",
    "\n",
    "cv2.namedWindow(\"Hsv detect\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "cv2.createTrackbar('H low', 'Hsv detect', 0, 179, lambda empty: empty)\n",
    "cv2.createTrackbar('S low', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "cv2.createTrackbar('V low', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "\n",
    "cv2.createTrackbar('H high', 'Hsv detect', 0, 179, lambda empty: empty)\n",
    "cv2.createTrackbar('S high', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "cv2.createTrackbar('V high', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "\n",
    "cv2.setTrackbarPos('H low', 'Hsv detect', 83)\n",
    "cv2.setTrackbarPos('S low', 'Hsv detect', 165)\n",
    "cv2.setTrackbarPos('V low', 'Hsv detect', 100)\n",
    "\n",
    "cv2.setTrackbarPos('H high', 'Hsv detect', 120)\n",
    "cv2.setTrackbarPos('S high', 'Hsv detect', 255)\n",
    "cv2.setTrackbarPos('V high', 'Hsv detect', 255)\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret, imag = cap.read()\n",
    "    \n",
    "    frame = cv2.cvtColor(imag, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Trackbars realtime position\n",
    "    h1 = cv2.getTrackbarPos('H low', 'Hsv detect')\n",
    "    s1 = cv2.getTrackbarPos('S low', 'Hsv detect')\n",
    "    v1 = cv2.getTrackbarPos('V low', 'Hsv detect')\n",
    "\n",
    "    h2 = cv2.getTrackbarPos('H high', 'Hsv detect')\n",
    "    s2 = cv2.getTrackbarPos('S high', 'Hsv detect')\n",
    "    v2 = cv2.getTrackbarPos('V high', 'Hsv detect')\n",
    "\n",
    "    color_low = np.array([h1, s1, v1], np.uint8)\n",
    "    color_high = np.array([h2, s2, v2], np.uint8)\n",
    "\n",
    "            \n",
    "    hsv = cv2.inRange(frame,  color_low, color_high)\n",
    "    \n",
    "    hsv_inv = cv2.bitwise_not(hsv)\n",
    "    \n",
    "    blue = np.zeros(frame.shape, np.uint8)\n",
    "\n",
    "    blue[:]=BLUE\n",
    "    \n",
    "    bg = cv2.bitwise_and(imag,imag,mask = hsv_inv)\n",
    "    fg = cv2.bitwise_and(blue,blue,mask = hsv)\n",
    "    \n",
    "    final = cv2.bitwise_or(bg, fg)\n",
    "   \n",
    "    cv2.imshow('Hsv detect', final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon extraction\n",
    "\n",
    "### Working principle\n",
    "Provided a mask of the colored areas, we need to the borders of the colored areas and transform those borders into simple polygons. \n",
    "\n",
    "This is done using OpenCV's findContours function ```contours, hierarchy = cv2.findContours(hsv, cv2.RETR_EXTERNAL  , cv2.CHAIN_APPROX_SIMPLE)``` The function uses an extended version of the border following algorithm. We configure the function so that it only returns external borders, thus eliminating holes in the mask and an approximation of the borders using as few points as possible. This is done in the function ```def find_color(frame, hsv_low, hsv_high)```\n",
    "\n",
    "Then, we discard the borders that are too small as they are likely to be falsely identified objects. The following condition ```if (cv2.contourArea(cnt) >= AREA_THRESH):``` allows us to keep only the borders big enough.\n",
    "The last step, is to merge the points that are too close together in order to create a low polygon approximation for each object. This is done in the function ```def cleanup_contours(contours, mode=0)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_contours(contours, mode=0):\n",
    "    #clean contours\n",
    "    AREA_THRESH = 100\n",
    "    MERGE_THRESH = 0.04\n",
    "    EPSILON = 40\n",
    "    \n",
    "    clean_contours = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        # only take big enough contours\n",
    "        if (cv2.contourArea(cnt) >= AREA_THRESH):\n",
    "            #convex hull\n",
    "            #hull = cv2.convexHull(cnt)\n",
    "            hull = cnt\n",
    "            #lower poly approx\n",
    "            if mode == 0:\n",
    "                epsilon = MERGE_THRESH*cv2.arcLength(hull,True)\n",
    "            else:\n",
    "                epsilon = EPSILON\n",
    "            approx = cv2.approxPolyDP(hull,epsilon,True)\n",
    "            \n",
    "            clean_contours.append(approx)\n",
    "            \n",
    "    return clean_contours\n",
    "\n",
    "\n",
    "def find_color(frame, hsv_low, hsv_high):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "\n",
    "    #Using inRange to find the desired range\n",
    "    mask = cv2.inRange(hsv,  hsv_low, hsv_high)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL  , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    \n",
    "    return cleanup_contours(contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test script\n",
    "In order to test the polygon extraction, we use the previously created sript with the polygon detection as addition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "     \n",
    "\n",
    "cv2.namedWindow(\"Hsv detect\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "cv2.createTrackbar('H low', 'Hsv detect', 0, 179, lambda empty: empty)\n",
    "cv2.createTrackbar('S low', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "cv2.createTrackbar('V low', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "\n",
    "cv2.createTrackbar('H high', 'Hsv detect', 0, 179, lambda empty: empty)\n",
    "cv2.createTrackbar('S high', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "cv2.createTrackbar('V high', 'Hsv detect', 0, 255, lambda empty: empty)\n",
    "\n",
    "cv2.setTrackbarPos('H low', 'Hsv detect', 83)\n",
    "cv2.setTrackbarPos('S low', 'Hsv detect', 165)\n",
    "cv2.setTrackbarPos('V low', 'Hsv detect', 100)\n",
    "\n",
    "cv2.setTrackbarPos('H high', 'Hsv detect', 120)\n",
    "cv2.setTrackbarPos('S high', 'Hsv detect', 255)\n",
    "cv2.setTrackbarPos('V high', 'Hsv detect', 255)\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Trackbars realtime position\n",
    "    h1 = cv2.getTrackbarPos('H low', 'Hsv detect')\n",
    "    s1 = cv2.getTrackbarPos('S low', 'Hsv detect')\n",
    "    v1 = cv2.getTrackbarPos('V low', 'Hsv detect')\n",
    "\n",
    "    h2 = cv2.getTrackbarPos('H high', 'Hsv detect')\n",
    "    s2 = cv2.getTrackbarPos('S high', 'Hsv detect')\n",
    "    v2 = cv2.getTrackbarPos('V high', 'Hsv detect')\n",
    "\n",
    "    color_low = np.array([h1, s1, v1], np.uint8)\n",
    "    color_high = np.array([h2, s2, v2], np.uint8)\n",
    "\n",
    "            \n",
    "    clean_contours = find_color(frame, color_low, color_high)\n",
    "    \n",
    "    cv2.drawContours(frame, clean_contours, -1, GREEN, 3)\n",
    "    \n",
    "    #draw points\n",
    "    for cnt in clean_contours:\n",
    "        for pt in cnt:\n",
    "            frame = cv2.circle(frame, (pt[0][0], pt[0][1]), radius=5, color=RED, thickness=-1)\n",
    "            \n",
    "    cv2.imshow('Hsv detect', frame)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot detection\n",
    "In order to simply find the robot's position, we place a blue isosceles triangle on top the the Thymio. The triangle must be facing forward and it's long side must be twice as long as it's short side. \n",
    "\n",
    "### Find the robot\n",
    "\n",
    "![image](images/MR_01.png)\n",
    "\n",
    "The idea for the robot detection is to find every polygon that is a triangle and sort them according to a score, The more the triangle has the right proportions, the lower score it gets. We then use the lowest score as the robot. \n",
    "\n",
    "\n",
    "$$Score = \\frac{||dAB-dCA||^2 + 2\\cdot{}||(dBC - dAB)||^2 + 2\\cdot{}||(dBC - dCA)||^2}{||dAB||^2}$$\n",
    "\n",
    "The score is calculated in python using the norm functions of numpy. ```score = (abs(dAB-dCA)+abs(K*dBC - dAB)+abs(K*dBC - dCA))/np.linalg.norm(dAB)```\n",
    "\n",
    "### Position, orientation and scale\n",
    "From the triangle's summits, we can find the robot's position, orientation and size. The position and orientation will be used for control and the size is used during the initialisation to find the image's scale. The position of the robot is computed as the mean value of the triangle's vertices. \n",
    "The angle is computed from the triangle direction vector using numpy's ```angle = np.arctan2(direction[1], direction[0])``` function. This function calculates the angle of a vector with the arctangeant of it's components while properly handling the singularity at an angle of $\\pi$. \n",
    "\n",
    "### The code\n",
    "The code takes an openCv standart frame as parameter as well as the scale of the world. It returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration\n",
    "BLUE_LOW  = [87, 129, 80]\n",
    "BLUE_HIGH = [131, 255, 255]\n",
    "\n",
    "def detect_robot(frame, scale=1):\n",
    "    blue_low = np.array(BLUE_LOW, np.uint8)\n",
    "    blue_high = np.array(BLUE_HIGH, np.uint8)\n",
    "    frame = frame.copy()\n",
    "    \n",
    "    clean_contours = find_color(frame, blue_low, blue_high)\n",
    "    \n",
    "    good_cnt = []\n",
    "    \n",
    "    for cnt in clean_contours:\n",
    "        if(len(cnt) == 3):\n",
    "            K = 2\n",
    "            A = 0\n",
    "            B = 0\n",
    "            C = 0\n",
    "            dAB = 0\n",
    "            dBC = 0\n",
    "            dCA = 0\n",
    "            p1 = cnt[0][0]\n",
    "            p2 = cnt[1][0]\n",
    "            p3 = cnt[2][0]\n",
    "            d1 = np.linalg.norm(p2-p1)\n",
    "            d2 = np.linalg.norm(p3-p2)\n",
    "            d3 = np.linalg.norm(p1-p3)\n",
    "            min_ix = np.argmin([d1, d2, d3])\n",
    "            if(min_ix == 0):\n",
    "                A = p3\n",
    "                B = p2\n",
    "                C = p1\n",
    "                dAB = d2\n",
    "                dBC = d1\n",
    "                dCA = d3\n",
    "            elif(min_ix == 1):\n",
    "                A = p1\n",
    "                B = p3\n",
    "                C = p2\n",
    "                dAB = d3\n",
    "                dBC = d2\n",
    "                dCA = d1\n",
    "            else:\n",
    "                A = p2\n",
    "                B = p3\n",
    "                C = p1\n",
    "                dAB = d2\n",
    "                dBC = d3\n",
    "                dCA = d1\n",
    "            score = abs(dAB-dCA)+abs(K*dBC - dAB)+abs(K*dBC - dCA)/np.linalg.norm(dAB)\n",
    "            good_cnt.append([A, B, C, score])\n",
    "                       \n",
    "    good_cnt = sorted(good_cnt, key = lambda x: x[3])\n",
    "    \n",
    "    robot_pos = [np.array([0, 0]), 0, False, 0]\n",
    "    \n",
    "    if(len(good_cnt) > 0):\n",
    "        robot_visible = True\n",
    "        A = good_cnt[0][0]\n",
    "        B = good_cnt[0][1]\n",
    "        C = good_cnt[0][2]\n",
    "        D = (np.mean([[B, C]], axis=1))[0]\n",
    "        \n",
    "        \n",
    "        Center = (np.mean([[A, B, C]], axis=1))[0]\n",
    "\n",
    "        direction = A - D\n",
    "        \n",
    "        size = np.linalg.norm(direction)\n",
    "        \n",
    "        angle = np.arctan2(direction[1], direction[0])\n",
    "        \n",
    "        frame = cv2.line(frame, (int(D[0]), int(D[1])), (int(A[0]), int(A[1])), color=BLUE, thickness=1)\n",
    "        frame = cv2.circle(frame, (int(Center[0]), int(Center[1])), radius=5, color=BLUE, thickness=-1)\n",
    "        Center = np.multiply(Center, scale).astype(int)\n",
    "        text =  \"position: ({:0.2f}, {:0.2f}) angle: {:0.4f}\".format(Center[0], Center[1], angle)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "        cv2.putText(frame, text, (10, 50), font, 0.5, GREEN, 1, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        robot_pos = [Center, angle, True, size]\n",
    "        \n",
    "    return robot_pos, frame\n",
    "\n",
    "frame = cv2.imread(\"images/colors.png\")\n",
    "\n",
    "pos, image = detect_robot(frame)\n",
    "\n",
    "\n",
    "print(\"Robot position object: [position, angle, visible, size in px]\")\n",
    "print(\"Robot position object:\", pos)\n",
    "    \n",
    "plt.imshow(image[:,:,::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obstacles detection\n",
    "The obstacles detection uses the polygon detection algorithm with the addition of the obstacles dilataion.\n",
    "\n",
    "### Dilatation\n",
    "The idea for the second dilatation method is to move the edges of each polygon outwards of a certain ammount to guarantee that the robot does not touch an edge. Additionally, we elongate the moved edges in order to cover part of the corners. Then, we move each vertex outwards to fully conver the corners. All these new verices maake the dilated obstacle.\n",
    "\n",
    "### Overlapping obstacles merge\n",
    "In some cases, after the dilatation, some obstacles that where aalready close might overlap. These overlapping obstacles should be merged into one. \n",
    "\n",
    "In order to do that, we draw the dilated polygons in white onto a black image. Then we run the contour finding alogorithm and openCV will find the merged dilated obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_LOW  = [150, 100, 100]\n",
    "RED_HIGH = [179, 255, 255]\n",
    "\n",
    "DIL_COEFF = 100\n",
    "EXP_RATIO = 60\n",
    "\n",
    "def detect_obstacles_alt(frame, scale=1):\n",
    "    frame = frame.copy()\n",
    "    red_low = np.array(RED_LOW, np.uint8)\n",
    "    red_high = np.array(RED_HIGH, np.uint8)\n",
    "    \n",
    "    clean_contours = find_color(frame, red_low, red_high)\n",
    "            \n",
    "    original_contours = []\n",
    "    dil_contour = []\n",
    "    for cnt in clean_contours:\n",
    "        mom = cv2.moments(cnt)\n",
    "        if mom[\"m00\"] != 0:\n",
    "            cx = int(mom[\"m10\"] / mom[\"m00\"])\n",
    "            cy = int(mom[\"m01\"] / mom[\"m00\"])\n",
    "            C = np.array([cx, cy])\n",
    "        else:\n",
    "            C = np.array([0, 0])\n",
    "        ncnt = []\n",
    "        ocnt = []\n",
    "        cnt = list(cnt)\n",
    "        cnt.append(cnt[0])\n",
    "        #print(cnt)\n",
    "        for i, _ in enumerate(cnt[0:-1]):\n",
    "            pt1 = cnt[i][0]\n",
    "            pt2 = cnt[i+1][0]\n",
    "            seg = pt2-pt1\n",
    "            d = seg/np.linalg.norm(seg)\n",
    "            n = np.array([-seg[1], seg[0]])/np.linalg.norm(seg)\n",
    "\n",
    "            N = pt1-C\n",
    "            N = N/np.linalg.norm(N)\n",
    "            npt = (pt1+(DIL_COEFF+EXP_RATIO/2)/scale*N).astype(int)\n",
    "\n",
    "            npt1 = (pt1+DIL_COEFF/scale*n - EXP_RATIO/scale*d).astype(int)\n",
    "            npt2 = (pt2+DIL_COEFF/scale*n + EXP_RATIO/scale*d).astype(int)\n",
    "\n",
    "            #frame = cv2.circle(frame, (npt[0], npt[1]), radius=5, color=(127, 0, 255), thickness=-1)\n",
    "            #frame = cv2.circle(frame, (npt1[0], npt1[1]), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            #frame = cv2.circle(frame, (npt2[0], npt2[1]), radius=5, color=(0, 127, 255), thickness=-1)\n",
    "\n",
    "            ncnt.append(npt)\n",
    "            ncnt.append(npt1)\n",
    "            ncnt.append(npt2)\n",
    "            ocnt.append(cnt[i][0])\n",
    "        \n",
    "        dil_contour.append(np.array(ncnt))\n",
    "        original_contours.append(np.multiply(ocnt, scale).astype(int))\n",
    "        \n",
    "        \n",
    "    \n",
    "    cv2.drawContours(frame, clean_contours, -1, (0,255,0), 3)\n",
    "    \n",
    "    \n",
    "    black = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    for i in range(len(dil_contour)):\n",
    "        cv2.drawContours(black, dil_contour, i, (255), -1)\n",
    "\n",
    "    plt.imshow(frame)\n",
    "    \n",
    "    #find contours\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(black, cv2.RETR_EXTERNAL  , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    clean_dil_contours = cleanup_contours(contours, 1)\n",
    "    cv2.drawContours(frame, clean_dil_contours, -1, (0,255,0), 3)\n",
    "\n",
    "    scaled_contours = []\n",
    "    for cnt in clean_dil_contours:\n",
    "        ncnt = []\n",
    "        for pt in cnt:\n",
    "            frame = cv2.circle(frame, (pt[0][0], pt[0][1]), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            ncnt.append(pt[0])\n",
    "        scaled_contours.append(np.multiply(ncnt, scale).astype(int))\n",
    "    \n",
    "    \n",
    "    return scaled_contours, original_contours, frame\n",
    "\n",
    "frame = cv2.imread(\"images/colors.png\")\n",
    "\n",
    "cont, ret, image = detect_obstacles_alt(frame)\n",
    "\n",
    "print(\"Contour object: \", cont)\n",
    "    \n",
    "plt.imshow(image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target detection\n",
    "The target detection uses the polygon detection to fond the conours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN_LOW  = [41, 64, 0]\n",
    "GREEN_HIGH = [85, 140, 140]\n",
    "def detect_targets(frame, scale=1):\n",
    "    frame = frame.copy()\n",
    "    green_low = np.array(GREEN_LOW, np.uint8)\n",
    "    green_high = np.array(GREEN_HIGH, np.uint8)\n",
    "    \n",
    "    clean_contours = find_color(frame, green_low, green_high)\n",
    "    \n",
    "    centroids = []\n",
    "    \n",
    "    for cnt in clean_contours:\n",
    "        mom = cv2.moments(cnt)\n",
    "        if mom[\"m00\"] != 0:\n",
    "            cx = int(mom[\"m10\"] / mom[\"m00\"])\n",
    "            cy = int(mom[\"m01\"] / mom[\"m00\"])\n",
    "            centroids.append([cx, cy])\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        \n",
    "            \n",
    "    cv2.drawContours(frame, clean_contours, -1, (0,255,0), 3)\n",
    "    scaled_centroids = []\n",
    "    \n",
    "    for pt in centroids:\n",
    "        frame = cv2.circle(frame, (pt[0], pt[1]), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "        scaled_centroids.append(np.multiply(pt, scale).astype(int).tolist())\n",
    "        \n",
    "    return scaled_centroids, frame\n",
    "\n",
    "frame = cv2.imread(\"images/colors.png\")\n",
    "\n",
    "targ, image = detect_targets(frame)\n",
    "    \n",
    "print(\"targets object: \", targ)\n",
    "plt.imshow(image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code encapsulation\n",
    "To provide a clean interface for the other modules of the project, the code has been rearranged in a class. This allows us to handle internal global variables in a clean way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entire class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global navigation\n",
    "\n",
    "The goal of this part is to create a trajectory that will allow the Thymio to pass through several interest points and come back to his initial position.\n",
    "\n",
    "The trajectory has to ensure that the Thymio avoid the global obstacles detected by the camera.\n",
    "\n",
    "\n",
    "## Path planning : Process flow\n",
    "\n",
    "The creation of our trajectory is composed of the following steps :\n",
    "\n",
    "- Dilatation of the obstacles to avoid collisions.\n",
    "\n",
    "- Visibility graph computation by using the previously dilated obstacles.\n",
    "\n",
    "- Path planning computation by using the visibility graph and the interest point positions.\n",
    "\n",
    "\n",
    "### Obstacle dilatation\n",
    "\n",
    "To perform the obstacle dilatation, we have implemented two different algorithms in parallel with the aim of taking the one that will provide the best results.\n",
    "\n",
    "The first algorithm that we used is based on the dilatation function provided in the OpenCV library :\n",
    "\n",
    "- Find the contours of the global obstacles.\n",
    "\n",
    "- Create a grayscale image with the global obstacles in white and the rest in black.\n",
    "\n",
    "- Dilate the global obstacles in this image.\n",
    "\n",
    "- Find the new dilated contours.\n",
    "\n",
    "For more details on the OpenCV dilatation function, see [Morphological transformations in OpenCV](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import sys\n",
    "import math\n",
    "#import numpy as np\n",
    "\n",
    "sys.path.append(\"../vision\")\n",
    "\n",
    "from vision import *\n",
    "\n",
    "#define\n",
    "X = 0\n",
    "Y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_obstacles(frame, scale=1):\n",
    "    \n",
    "    # Find the contours of the global obstacles\n",
    "    frame = frame.copy()\n",
    "    red_low = np.array(RED_LOW, np.uint8)\n",
    "    red_high = np.array(RED_HIGH, np.uint8)\n",
    "    \n",
    "    clean_contours = find_color(frame, red_low, red_high)\n",
    "    \n",
    "    \n",
    "    # Some operations used for plotting purposes and scalling of the image in real dimmensions\n",
    "    cv2.drawContours(frame, clean_contours, -1, (0,255,0), 3)\n",
    "            \n",
    "    original_contours = []\n",
    "    for cnt in clean_contours:\n",
    "        ocnt = []\n",
    "        for pt in cnt:\n",
    "            ocnt.append(pt[0])\n",
    "        original_contours.append(np.multiply(ocnt, scale).astype(int))\n",
    "        \n",
    "        \n",
    "    # Create a grayscale image with the global obstacles in white and the rest in black\n",
    "    black = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    for i in range(len(clean_contours)):\n",
    "        cv2.drawContours(black, clean_contours, i, (255), -1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(black, cmap=\"gray\")\n",
    "    \n",
    "    \n",
    "    # Dilate the global obstacles in this image\n",
    "    kernel = np.ones((DIL_COEFF_K,DIL_COEFF_K),np.uint8)\n",
    "    black = cv2.dilate(black, kernel, iterations = 15)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(black, cmap=\"gray\")\n",
    "    \n",
    "    \n",
    "    # Find the new dilated contours\n",
    "    contours, hierarchy = cv2.findContours(black, cv2.RETR_EXTERNAL  , cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    clean_dil_contours = cleanup_contours(contours, 1)\n",
    "\n",
    "    \n",
    "    # Some operations used for plotting purposes and scalling of the image in real dimmensions\n",
    "    scaled_contours = []\n",
    "    for cnt in clean_dil_contours:\n",
    "        ncnt = []\n",
    "        for pt in cnt:\n",
    "            frame = cv2.circle(frame, (pt[0][0], pt[0][1]), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            ncnt.append(pt[0])\n",
    "\n",
    "        scaled_contours.append(np.multiply(ncnt, scale).astype(int))\n",
    "    \n",
    "    \n",
    "    return scaled_contours, original_contours, frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the result of our algorithm, we will use the following function (note that this function will be also used to plot other results computed in the global Navigation part) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGlobalNavigation(contours, contoursMapped, possibleDisplacement = {}, interestPoints = [], trajectory = []):\n",
    "    \"\"\"Plot the original contours and the dilated contours using matplotlib\n",
    "       Plot the visibility graph if possibleDisplacement is given \n",
    "       Plot the Thymio's point of interest if interestPoints is given\n",
    "       Plot the Thymio's path if the trajectory is given\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contours : list of list of list\n",
    "        The camera detect several obstacles\n",
    "        Each obstacle has several extremities\n",
    "        Each extremity has (x, y) coordinates\n",
    "\n",
    "    contoursMapped : list of list of list\n",
    "        Same structure as contours, each extremity's coordinate has been dilated\n",
    "\n",
    "    possibleDispacement : dictionary\n",
    "        Each extremity point has several visible points, i.e possible destinations for the Thymio\n",
    "\n",
    "    interestPoints : list of list\n",
    "        Each point of interest has (x, y) coordinates, i.e locations where the thymio need to go\n",
    "\n",
    "    trajectory : list of list\n",
    "        Each point of the trajectory has (x, y) coordinates\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    xOriginal = []\n",
    "    yOriginal = []\n",
    "    xDilated = []\n",
    "    yDilated = []\n",
    "\n",
    "    for obstacleOriginal in contours:\n",
    "        for extremityOriginal in obstacleOriginal:\n",
    "            xOriginal.append(extremityOriginal[X])\n",
    "            yOriginal.append(extremityOriginal[Y])\n",
    "\n",
    "        xOriginal.append(obstacleOriginal[0][X])\n",
    "        yOriginal.append(obstacleOriginal[0][Y])\n",
    "\n",
    "        plt.plot(xOriginal, yOriginal, 'b')\n",
    "\n",
    "        xOriginal.clear()\n",
    "        yOriginal.clear()\n",
    "\n",
    "    \n",
    "    for obstacleDilated in contoursMapped:\n",
    "        for extremityDilated in obstacleDilated:\n",
    "            xDilated.append(extremityDilated[X])\n",
    "            yDilated.append(extremityDilated[Y])\n",
    "\n",
    "        xDilated.append(obstacleDilated[0][X])\n",
    "        yDilated.append(obstacleDilated[0][Y])\n",
    "\n",
    "        plt.plot(xDilated, yDilated, 'm')\n",
    "\n",
    "        xDilated.clear()\n",
    "        yDilated.clear()\n",
    "\n",
    "\n",
    "    if possibleDisplacement:\n",
    "        for extremity in possibleDisplacement:\n",
    "            for visiblePoint in possibleDisplacement[extremity]:\n",
    "                plt.plot([extremity[X], visiblePoint[X]], [extremity[Y], visiblePoint[Y]], 'm')\n",
    "\n",
    "    \n",
    "    if interestPoints:\n",
    "        for point in interestPoints:\n",
    "            plt.plot([point[X]], [point[Y]], 'kx', markersize=12)\n",
    "\n",
    "\n",
    "    if trajectory:\n",
    "        for i in range (1, len(trajectory)):\n",
    "            plt.arrow(trajectory[i-1][X], trajectory[i-1][Y], trajectory[i][X] - trajectory[i-1][X], trajectory[i][Y] - trajectory[i-1][Y], head_width=8, length_includes_head=True, color  = 'k', width = 2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(\"./images/colors.png\")\n",
    "\n",
    "# Dilate obstacles and print them\n",
    "scaled_contours, original_contours, ret = detect_obstacles(frame)\n",
    "\n",
    "plt.figure()\n",
    "plt.gca().invert_yaxis()\n",
    "printGlobalNavigation(original_contours, scaled_contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICI MONTRER L'AUTRE METHODE + conclusion sur le choix de la methode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visibility graph computation\n",
    "\n",
    "Remark : Here I will explain in details how visibility graph algorithms work as i think that this is the most important part of the global navigation subsystem.\n",
    "\n",
    "#### Naive algorithm\n",
    "\n",
    "Reference : [Naive visibility graph algorithm](https://taipanrex.github.io/2016/09/17/Distance-Tables-Part-1-Defining-the-Problem.html)\n",
    "\n",
    "Computational Geometry defines visibility graphs in the following way: Given a set $S$ of disjoint polygonal obstacles, we denote the visibility graph $G_{vis}(S)$. It’s nodes are the vertices of $S$ and there is an arc between vertices $v$ and $w$ if they can see each other, that is, if the segment $\\overline{vw}$ does not intersect the interior of any obstacle in $S$.\n",
    "\n",
    "Here is the algorithm :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G <-- all vertices of S\n",
    "VG <-- empty visibility graph\n",
    "for each vertex v in G                                            #O(n)\n",
    "    for each vertex w in {G - v}                                  #O(n)\n",
    "        for each edge e in S                                      #O(e)\n",
    "            if the arc from v to w does not intersect any edge e then\n",
    "            vertex v and w are visible to each other\n",
    "                VG <-- edge v to w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the visibility graph naïvely, we add all the vertices from our set of obstacles to visibility graph G. For each vertex v in G, we check it against all the other vertices in G to see which vertices are visible to v. To check if a vertex w is visible, we need to check if the arc/edge from v to w intersects with any of the edges of the obstacles. If it doesn’t intersect any edges, v is visible to w and vice versa. There is no obstacle blocking the view between v and w and it can be used as part of a path.\n",
    "\n",
    "\n",
    "The issue with the naïve algorithm is that it’s time complexity is $O(n^3)$. A better algorithm is the D.T.Lee's visibility graph algorithm which runs in $O(n^2log_2n)$.\n",
    "\n",
    "\n",
    "#### Lee's visibility graph algorithm\n",
    "\n",
    "Reference : [Lee's visibility graph algorithm](https://taipanrex.github.io/2016/10/19/Distance-Tables-Part-2-Lees-Visibility-Graph-Algorithm.html)\n",
    "\n",
    "We are still going to need the first two for loops as in the naïve solution detailed in the previous part. Lee’s approach saves us running time by reducing the number of edges we need to check for each pair of points. That part of Lee’s algorithm runs in $O(log_2n)$ time, leaving a total running time of $O(n^2log_2n)$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "visibility_graph(S <-- disjoint polygonal obstacles)\n",
    "    G <-- all vertices of S\n",
    "    VG -> empty visibility graph\n",
    "    for each vertex v in G                                        #O(n)\n",
    "        do VG <-- visible_vertices(v,S)                           #O(n log n)\n",
    "    return VG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the visible_vertices function, a key concept to understand in Lee’s algorithm is the scan line.\n",
    "\n",
    "![](lee_figure1.png)\n",
    "\n",
    "Let’s say we are checking which points are visible from point $s$. To do this we need to visit each of the points $a$ through $f$. The way we are going to visit the points is in a counter clockwise circle. We are going to use Lee’s scan line for this, which is a half-line. Conceptually the scan line has its origin at point $s$, pointing to the right (parallel to the x-axis) and moves counter clock wise until it hits a point to check for visibility.\n",
    "\n",
    "Together with the scan line we are going to keep a ordered list of edges that we will need to use when we visit each point. we call this the $open\\_edges$ list. This list will be used to check for point visibility.\n",
    "\n",
    "Take figure 1: the first point the scan line will hit is point $a$, which has two edges (edge $ab$ and edge $ac$). What we do is check if each edge is on the “counter clock wise” side of the scan line. I.e., when the scan line continues moving, will it intersect any of those edges? In the case of point $a$, both edges are on the CCW side and will be added the $open\\_edges$ list I mentioned we are tracking.\n",
    "\n",
    "Lets continue the scan line to point $b$ (figure 2). Now, edge $ab$ is on the clock wise side of the scan line and **it will never be intersected by the scan line again**. This means we are free to completely ignore that edge for all unvisited points and we can remove it from the tracking $open\\_edges$ list. Edge $ac$ is still partially on the CCW side and as the scan line continues to move, it will continue to intersect edge $ac$, so it stays in the list. edge $bc$ should now be added to $open\\_edges$ as it is on the CCW side and will be intersected by the scan line. So for each point the scan line visits, we check the edges incident at that point. If the edge is on the CCW side, we add it to $open\\_edges$. If the edge is on the CW side, we remove it from $open\\_edges$.\n",
    "\n",
    "Lets now discuss visibility, using figure 1 and 2. When the scan line visits point $a$, it will check the $open\\_edges$ list to see if there are any edges that could possibly block visibility. At point $a$ there are none so $a$ is visible. Moving to point $b$, $open\\_edges$ contains edge $ac$ and the line from point $s$ to point $b$ intersects edge $ac$. Point $b$ is therefore not visible.\n",
    "\n",
    "As illustrated, what the scan line allows us to do is ignore edges that are no longer an issue, i.e. edges that can no longer block visibility of the next points to visit. When the scan line moves on to point $c$, $d$, $e$ and $f$, it will never have to consider the edges that it has already passed, like edge $ab$. **The naïve algorithm would have to check all edges, Lee’s algorithm only checks relevant edges.**\n",
    "\n",
    "As a matter of fact, we only need to check the closest $open\\_edges$ edge. Take figure 3 below: when the scan line visits point $x$, $open\\_edges$ will contain all the left and right edges of the three triangles. We don’t need to loop through $open\\_edges$ and check if we intersect an edge, we only need to check the edge with the closest intersect point from $s$ (i.e. the left most edge in this case).\n",
    "\n",
    "![](lee_figure3.png)\n",
    "\n",
    "To achieve this, we need to keep $open\\_edges$ ordered by the intersect distance on the scan line from $s$. We achieve this using a binary search tree, which allows us to look up the closest open edge in $O(log_2n)$ time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "visible_vertices(v, S):\n",
    "    1. sort the vertices of the obstacle polygons according to the\n",
    "    counter clock wise angle the half line from v to each vertex makes\n",
    "    with the x-axis. In the case of ties, vertices closer to v should\n",
    "    come first. Let w_i, ..., w_n be this list.\n",
    "    2. let s be the scan line (half line) starting at v, parallel with\n",
    "    the x-axis, extending to positive infinity. Check all edges of\n",
    "    S for intersection with s and store intersected edges in a binary\n",
    "    search tree T.\n",
    "    3. W <-- empty list of visible vertices\n",
    "    4. for i <-- 1 to n vertices\n",
    "    5.     do if visible(w_i) then add w_i to W\n",
    "    6.     Insert into T the edges incident to w_i that lie on the counter\n",
    "           clock wise side of scan line s.\n",
    "    7.     Delete from T the edges incident to w_i that lie on the clock\n",
    "           wise side of scan line s.\n",
    "    8. Return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 1, we order all the points we are going to visit in the order the scan line will hit the points, moving in a counter clock wise direction.\n",
    "\n",
    "In step 2, we initialize $open\\_edges$. It is important to do this before we start visiting all the points; figure 3 illustrates the reason for this. In figure 3, the first point the scan line hits is $x$. If we do not perform step 2, $open\\_edges$ will be empty and we would think that $x$ is visible. So in the initialization step, we need to check all obstacle edges and store the edges that intersect the horizontal scan line. This step takes $O(nlog_2n)$\n",
    "(checking n edges, where inserting into the binary search tree costs $O(log_2n)$).\n",
    "\n",
    "In steps 4 to 7, we visit each of the obstacle points and check for visibility. We also keep $open\\_edges$ updated."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "visible(w_i):\n",
    "    1. If T is empty then return True\n",
    "    2. else if the edge from v to w_i does not intersect the smallest\n",
    "       (left-most) edge in T then return True\n",
    "    3. else return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 1, if there are no edges in $open\\_edges$, then $w\\_i$ is visible. In step 2, $open\\_edges$ is not empty, so we need to check the “smallest” or edge that has the shortest distance to the intersection point with $v$ to $w\\_i$. In a binary search tree, that will be the left-most node. If $v$ to $w\\_i$ intersects this line, then $w\\_i$ is not visible.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "In order to compute the visibility graph, we use the MIT-licensed Python package named Pyvisgraph. For more details on this library, please check the GitHub page [Pyvisgraph](https://github.com/TaipanRex/pyvisgraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyvisgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvisgraph as vg\n",
    "\n",
    "def computeVisibilityGraph(contoursMapped):\n",
    "    \"\"\"Given the dilated obstacles, compute the visibility graph\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contoursMapped : list of list of list\n",
    "        Same structure as contours, each extremity's coordinate has been dilated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    g : object of class Graph\n",
    "        the visibility graph of our problem\n",
    "\n",
    "    possibleDisplacement : dictionary\n",
    "        key : tuple containing (x,y) coordinates of one of the edges of the dilated obstacles\n",
    "        value : list of all other edges visible from the key edge\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the visibility graph\n",
    "    polys = [[] for _ in contoursMapped]\n",
    "\n",
    "    for obstacle, i in zip(contoursMapped, range(len(contoursMapped))):\n",
    "        for extremity in obstacle:\n",
    "            polys[i].append(vg.Point(extremity[X], extremity[Y]))\n",
    "\n",
    "    g = vg.VisGraph()\n",
    "    g.build(polys)\n",
    "\n",
    "\n",
    "    # Create a dictionary where each extremety point has several visible points i.e possible destinations\n",
    "    possibleDisplacement = {}\n",
    "\n",
    "    for obstacle, i in zip(contoursMapped, range(len(contoursMapped))):\n",
    "        for extremity, j in zip(obstacle, range(len(obstacle))):\n",
    "            visible = g.find_visible(polys[i][j])\n",
    "            possibleDisplacement[(extremity[X], extremity[Y])] = [[point.x, point.y] for point in visible]\n",
    "            visible.clear()\n",
    "\n",
    "    return g, possibleDisplacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result by plotting the visibility graph computed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the visibility graph and print it\n",
    "g, possibleDisplacement = computeVisibilityGraph(scaled_contours)\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.gca().invert_yaxis()\n",
    "printGlobalNavigation(original_contours, scaled_contours, possibleDisplacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path planning\n",
    "\n",
    "In our project we want the Thymio to pass through several point of interest and come back to his initial position. We thus have something very similar to the Travelling salesman problem.\n",
    "\n",
    "In one of the exercise cession, we learnt how to compute a solution to this problem using genetic algorithms (see [Evolution of a salesman: A complete genetic algorithm tutorial for Python](https://towardsdatascience.com/evolution-of-a-salesman-a-complete-genetic-algorithm-tutorial-for-python-6fe5d2b3ca35)).\n",
    "We also saw how we could adapt the genetic algorithm to a situation more suited for a robot, i.e with obstacles for exemple. \n",
    "\n",
    "However, the Pyvisgraph library that we used already contains a shortest path algorithm which is suited with the visibility graph computed previously. Indeed, it is implemented such that we can easily find the shortest path not only from two vertices of the visibility graph but also from two points that are not already in the visibility graph, which is always the case in our situation. The algorithm used by the library is the Dijkstra's algorithm. We won't go deep in the details as this algorithm has been already see in class.\n",
    "\n",
    "To compute our trajectory we used the following steps :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "computeTrajectory(visibilityGraph, interestPoints):\n",
    "    1. Let Path be an empty list \n",
    "    2. While there are still some points to travel to\n",
    "    3.    Find the closest interest point to the current position\n",
    "    4.    Compute an optimal path from the current position to this closest interest point using Dijksta's algorithm\n",
    "    5.    Add to Path all the intermediary point computed by the dijkstra's algorithm\n",
    "    6.    Update the current position to be the closest interest point found previously\n",
    "    \n",
    "    7. Return Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice that this algorithm does not necessarily return a trajectory which minimises the total distance travelled, but ensures to return a trajectory where the path between two interest points is optimal. Moreover, each time we look for the closest point, we use a simple euclidean norm-2 without considerating the obstacles that could be between the points.\n",
    "\n",
    "However, we believe that the trajectory returned by our algorithm is often the optimal one or at least close to the optimal one. The algorithm is also way faster than a genetic algorithm, much simpler and reuse a shortest path algorithm already implemented and optimised to work well with a visibility graph.\n",
    "\n",
    "Here is the implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTrajectory(graph, interestPoints): \n",
    "    \"\"\"Given the visibility graph and the points of interest\n",
    "       Compute a path planning allowing the Thymio to pass through all the point of interest\n",
    "       and come back to his initial position\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    g : object of class Graph\n",
    "        the visibility graph of our problem\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path : list of list\n",
    "        Each point defining the trajectory has (x, y) coordinates\n",
    "        The final trajectory is basically a line joining all point of the path\n",
    "    \"\"\"\n",
    "    \n",
    "    startingPoint = interestPoints[0]\n",
    "    pointTravelled2 = [startingPoint]\n",
    "    path = [startingPoint]\n",
    "    interestPointsLeft = [x for x in interestPoints if x != startingPoint]\n",
    "    i = 0\n",
    "\n",
    "    while i != len(interestPoints):\n",
    "\n",
    "        index = -1\n",
    "        minimum = np.inf\n",
    "        point = pointTravelled2[i]\n",
    "\n",
    "        # find the closest interest point to the current interest point\n",
    "        if i != len(interestPoints) - 1:\n",
    "            for pointLeft, j in zip(interestPointsLeft, range(len(interestPointsLeft))):\n",
    "                dist = math.sqrt((point[X] - pointLeft[X])**2 + (point[Y] - pointLeft[Y])**2)\n",
    "                if dist < minimum:\n",
    "                    minimum = dist\n",
    "                    index = j\n",
    "\n",
    "        # if there is no remaining point of interest, we need to come back to the starting point\n",
    "        else:\n",
    "            interestPointsLeft.append(startingPoint)\n",
    "            index = 0\n",
    "\n",
    "        # compute an optimal path from the current interest point to his closest interest point using the visibility graph\n",
    "        # and add the points of this new optimal path in the total path\n",
    "        shortest = graph.shortest_path(vg.Point(point[X], point[Y]), vg.Point(interestPointsLeft[index][X], interestPointsLeft[index][Y]))\n",
    "        for j in range(1, len(shortest)):\n",
    "            path.append([shortest[j].x, shortest[j].y])\n",
    "\n",
    "        # remove the closest interest point as we finish to explore it\n",
    "        pointTravelled2.append([interestPointsLeft[index][X], interestPointsLeft[index][Y]])\n",
    "        interestPointsLeft.remove([interestPointsLeft[index][X], interestPointsLeft[index][Y]])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result by plotting the trajectory :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trajectory going through all the points of interest and going back to the starting point\n",
    "interestPoints = [[1393, 856], [1212, 1024], [540, 697], [1735, 673], [1003, 532], [1707, 238], [385, 171]]\n",
    "\n",
    "trajectory = computeTrajectory(g, interestPoints)\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.gca().invert_yaxis()\n",
    "printGlobalNavigation(original_contours, scaled_contours, interestPoints = interestPoints, trajectory = trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
